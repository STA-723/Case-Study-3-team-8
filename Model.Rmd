---
title: "Model"
author: "Raphaël Morsomme"
date: "February 11, 2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(randomForest)
library(xtable)
library(tidyverse)
load("Data/data_model.RDATA")
d <- d_cheap

my_ggsave <- partial(ggsave, path = "Deliverables/Figures", width = 5, height = 3.5)

generate_table <- function(d, name = "test.tex", cap = ""){
  d %>%
    xtable(caption= cap) %>%
    print(file= str_c("Deliverables/Figures/", name), type = "latex", booktab = TRUE)
}
```


```{r model fitting}
m <- randomForest(Y ~ . - consumption_risk - awareness - situational_risk, data = d, importance = TRUE,
                  do.trace = 10,
                  
                  # Computational costs
                  ntree = 1e2,
                  
                  # Sensitivity analysis
                  mtry = floor(ncol(d)/3),
                  nodesize = 5, # pruning
                  maxnodes = NULL) # pruning
```


```{r model output}
m %>%
  importance() %>%
  
  as_tibble(rownames = "Variables") %>%
  rename(Importance = `%IncMSE`) %>%
  select(Variables, Importance) %>%
  arrange(-Importance) %>%
  
  generate_table(name = "example.tex", cap = "Example")
```

```{r sample}
mtcars %>%
  ggplot(aes(x = mpg)) +
  geom_histogram()
my_ggsave("test.jpeg")
```


## Conformal Prediction

```{r conformal}
# Setup
output <- tibble(Width = numeric(0),
                 Coverage   = numeric(0),
                 Significance = numeric(0))

signifs <- c(0.95, 0.9, 0.75, 0.5)
prop_test <- 1/10
prop_callibration <- 1/3

# Size of test, callibration and proper training set
n <- nrow(d)
n_test  <- n %/% (1/prop_test)
n_train <- n - n_test
n_callibration_max <- n_train %/% (1/prop_callibration)
n_callibration     <- 20*(n_callibration_max %/% 20)
n_train_proper     <- n_train - n_callibration 
n == n_train_proper + n_callibration + n_test # sanity check
n_train == n_train_proper + n_callibration # sanity check

# repeat experiment N times
N <- 1e1

for(i in 1 : N){
  print(i)
  
  d_random <- d %>% sample_frac() # shuffle data
  d_train_proper <- d_random %>% slice(1:n_train_proper)
  d_callibration <- d_random %>% slice(((n_train_proper+1):n_train))
  d_test         <- d_random %>% slice(((n_train+1):n))
  nrow(d) == nrow(d_train_proper) + nrow(d_callibration) + nrow(d_test) # Sanity check
  
  # Fit model to proper training set
  m <- randomForest(Y ~ ., data = d_train_proper)

  # Compute anomaly score on callibration set
  pred_callibration <- predict(m, d_callibration)
  a_callibration <- abs(pred_callibration - d_callibration$Y) %>% sort

  # Prediction intervals on test set
  d_test <- d_test %>% mutate(y_hat = predict(m, d_test))

  for(e in signifs){
    a_cutoff <- a_callibration[n_callibration*e]
  
    d_test <- d_test %>% 
      mutate(bound_low = y_hat - a_cutoff,
             bound_upp = y_hat + a_cutoff,
             Width = bound_upp - bound_low,
             Coverage = (bound_low <= Y) & (Y <= bound_upp),
             Significance = e)
  
    output <- rbind(output, d_test %>% select(Width, Coverage, Significance)) # save output
  } # end-loop signifs
} # end-loop N

save(output, file = "Data/conformal.RDATA")
```


```{r conformal output}
output %>%
  group_by(Significance) %>%
  summarize_all(mean) %>%
  rename(`Mean Width` = Width) %>%
  generate_table(name = "conformal.tex", cap = "Coverage and Mean Width of Prediction Intervals")

output %>%
  group_by(signif) %>%
  summarize(q10 = width %>% quantile(0.1),
            q90 = width %>% quantile(0.9),
            q50 = width %>% quantile(0.5)) %>%
  ggplot(aes(x = signif, y = q50)) +
  geom_errorbar(aes(ymin = q10, ymax = q90)) +
  geom_point() +
  geom_line(linetype = 2) +
  labs(y = "Width", x = "Significance Level") +
  theme_bw()
my_ggsave("conformal.jpeg")
```

